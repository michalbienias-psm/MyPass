{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12748433,"sourceType":"datasetVersion","datasetId":8058856}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Find the CSV, load it, and print a quick preview\n\nimport os\nimport re, unicodedata, pandas as pd\nfrom datetime import date\n\n# Discover CSVs under /kaggle/input\ncsv_paths = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename.lower().endswith('.csv'):\n            csv_paths.append(os.path.join(dirname, filename))\n\nif not csv_paths:\n    raise FileNotFoundError(\"No CSV files found under /kaggle/input. Double-check your dataset reference.\")\n\n# If there are multiple, just take the first for now (you can pick a specific one if needed)\ncsv_path = csv_paths[0]\nprint(\"Using CSV:\", csv_path)\n\n# Load (utf-8-sig handles BOMs from Excel exports nicely)\ndf = pd.read_csv(csv_path, encoding=\"utf-8-sig\", low_memory=False)\n\nprint(\"Shape:\", df.shape)\nprint(\"Columns:\", list(df.columns))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-14T06:24:31.369982Z","iopub.execute_input":"2025-08-14T06:24:31.370347Z","iopub.status.idle":"2025-08-14T06:24:31.419350Z","shell.execute_reply.started":"2025-08-14T06:24:31.370323Z","shell.execute_reply":"2025-08-14T06:24:31.417224Z"}},"outputs":[{"name":"stdout","text":"Using CSV: /kaggle/input/psm-members/members.csv\nShape: (1515, 14)\nColumns: ['Submission Time', 'First Name', 'Last Name', 'Birthday', 'E-mail', 'Phone Number', 'Select an Address', 'I understand that all personal data is securely stored on the servers of the Polish Youth Association and is not shared with other institutions.', 'I agree to be kept informed about updates from the Polish Youth Association, including events, scholarships, and the latest news related to youth.', 'Adres korespondencyjny', 'ID', 'Owner', 'Created Date', 'Updated Date']\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"# Keep only the requested columns (exact header names)\nKEEP_COLS = [\n    \"First Name\",\n    \"Last Name\",\n    \"Birthday\",\n    \"E-mail\",\n    \"Phone Number\",\n    \"Select an Address\",\n    \"Adres korespondencyjny\",\n]\n\nmissing = [c for c in KEEP_COLS if c not in df.columns]\nif missing:\n    print(\"WARNING: Missing columns:\", missing)\n\nexisting = [c for c in KEEP_COLS if c in df.columns]\ndf_trim = df[existing].copy()\n\nprint(\"Trimmed shape:\", df_trim.shape)\nprint(\"Trimmed columns:\", list(df_trim.columns))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T06:24:31.422098Z","iopub.execute_input":"2025-08-14T06:24:31.422527Z","iopub.status.idle":"2025-08-14T06:24:31.436455Z","shell.execute_reply.started":"2025-08-14T06:24:31.422494Z","shell.execute_reply":"2025-08-14T06:24:31.434910Z"}},"outputs":[{"name":"stdout","text":"Trimmed shape: (1515, 7)\nTrimmed columns: ['First Name', 'Last Name', 'Birthday', 'E-mail', 'Phone Number', 'Select an Address', 'Adres korespondencyjny']\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"SAA = \"Select an Address\"\nADR = \"Adres korespondencyjny\"\n\n# Normalize whitespace / empties\nfor col in [SAA, ADR]:\n    if col in df_trim.columns:\n        df_trim[col] = (\n            df_trim[col]\n            .astype(str)\n            .str.strip()\n            .replace({\"\": pd.NA, \"nan\": pd.NA, \"NaN\": pd.NA})\n        )\n\n# Fill Select an Address from Adres korespondencyjny where SAA is empty and ADR has a value\nmask = df_trim[SAA].isna() & df_trim[ADR].notna()\nfilled_count = int(mask.sum())\ndf_trim.loc[mask, SAA] = df_trim.loc[mask, ADR]\n\nprint(f\"Filled {filled_count} empty '{SAA}' cells from '{ADR}'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T06:24:31.439345Z","iopub.execute_input":"2025-08-14T06:24:31.439938Z","iopub.status.idle":"2025-08-14T06:24:31.497292Z","shell.execute_reply.started":"2025-08-14T06:24:31.439905Z","shell.execute_reply":"2025-08-14T06:24:31.495638Z"}},"outputs":[{"name":"stdout","text":"Filled 574 empty 'Select an Address' cells from 'Adres korespondencyjny'.\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"def nonempty(series: pd.Series) -> pd.Series:\n    return (\n        series.astype(str)\n        .str.strip()\n        .replace({\"\": pd.NA, \"nan\": pd.NA, \"NaN\": pd.NA})\n        .notna()\n    )\n\n# 1) Totals (post-sync)\ntotal_members = len(df_trim)\nafter_filled = int(nonempty(df_trim[SAA]).sum())\n\nprint(f\"Total members (rows): {total_members}\")\nprint(f\"Members with '{SAA}' filled AFTER sync: {after_filled} ({after_filled/total_members:.1%})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T06:24:31.498593Z","iopub.execute_input":"2025-08-14T06:24:31.499177Z","iopub.status.idle":"2025-08-14T06:24:31.514957Z","shell.execute_reply.started":"2025-08-14T06:24:31.499147Z","shell.execute_reply":"2025-08-14T06:24:31.513188Z"}},"outputs":[{"name":"stdout","text":"Total members (rows): 1515\nMembers with 'Select an Address' filled AFTER sync: 1515 (100.0%)\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"EMAIL_COL = \"E-mail\"  # exact header in your CSV\n\ndef dup_summary(series: pd.Series):\n    s = series.fillna(\"\").astype(str).str.strip()\n    # counts per key\n    vc = s[s != \"\"].value_counts()\n    dupe_keys = vc[vc > 1]\n    rows_in_dupes = int(s.isin(dupe_keys.index).sum())\n    unique_nonempty = s.replace(\"\", pd.NA).nunique(dropna=True)\n    return unique_nonempty, dupe_keys, rows_in_dupes\n\n# Raw (case-sensitive)\nraw_unique, raw_dupe_keys, raw_rows_in_dupes = dup_summary(df_trim[EMAIL_COL])\n\n# Case-insensitive (lowercased)\nlower_unique, lower_dupe_keys, lower_rows_in_dupes = dup_summary(df_trim[EMAIL_COL].str.lower())\n\ntotal = len(df_trim)\n\nprint(\"=== Email duplicate check ===\")\nprint(f\"Total rows: {total}\")\nprint(\"\\n-- Exact match (case-sensitive) --\")\nprint(f\"Unique emails: {raw_unique}\")\nprint(f\"Duplicate email KEYS (count>1): {len(raw_dupe_keys)}\")\nprint(f\"Rows that belong to duplicate groups: {raw_rows_in_dupes}\")\n\nprint(\"\\n-- Case-insensitive (lowercased) --\")\nprint(f\"Unique emails: {lower_unique}\")\nprint(f\"Duplicate email KEYS (count>1): {len(lower_dupe_keys)}\")\nprint(f\"Rows that belong to duplicate groups: {lower_rows_in_dupes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T06:24:31.518906Z","iopub.execute_input":"2025-08-14T06:24:31.519379Z","iopub.status.idle":"2025-08-14T06:24:31.568104Z","shell.execute_reply.started":"2025-08-14T06:24:31.519350Z","shell.execute_reply":"2025-08-14T06:24:31.566336Z"}},"outputs":[{"name":"stdout","text":"=== Email duplicate check ===\nTotal rows: 1515\n\n-- Exact match (case-sensitive) --\nUnique emails: 1286\nDuplicate email KEYS (count>1): 174\nRows that belong to duplicate groups: 402\n\n-- Case-insensitive (lowercased) --\nUnique emails: 1282\nDuplicate email KEYS (count>1): 174\nRows that belong to duplicate groups: 406\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"EMAIL = \"E-mail\"\nBDAY  = \"Birthday\"\n\n# 1) Normalize keys for grouping\ndf_dup = df_trim.copy()\ndf_dup[\"_email_lower\"] = df_dup[EMAIL].astype(str).str.strip().str.lower()\ndf_dup[\"_bday_dt\"] = pd.to_datetime(df_dup[BDAY], errors=\"coerce\").dt.date  # date only\n\n# 2) Keep only rows with a non-empty email\ndf_dup = df_dup[df_dup[\"_email_lower\"].str.len() > 0]\n\n# 3) Consider only emails that appear more than once (case-insensitive)\ngrp = df_dup.groupby(\"_email_lower\", dropna=False)\ndup_emails = grp.size()\ndup_emails = dup_emails[dup_emails > 1].sort_values(ascending=False)\n\nprint(f\"Total rows: {len(df_trim)}\")\nprint(f\"Duplicate emails (case-insensitive): {len(dup_emails)}\")\n\n# 4) Summarize birthdays within each duplicate email group\ndef summarize_group(g):\n    # unique non-null birthdays\n    uniq_bdays = sorted({d for d in g[\"_bday_dt\"].dropna().tolist()})\n    n_nonnull  = len(uniq_bdays)\n    status = (\n        \"conflict_birthdays\" if n_nonnull > 1 else\n        (\"same_birthday\" if n_nonnull == 1 else \"no_birthday_data\")\n    )\n    return pd.Series({\n        \"rows_in_group\": len(g),\n        \"distinct_birthdays\": n_nonnull,\n        \"birthdays_list\": \", \".join(map(str, uniq_bdays)) if uniq_bdays else \"\",\n        \"status\": status,\n    })\n\ndup_summary = (\n    df_dup[df_dup[\"_email_lower\"].isin(dup_emails.index)]\n    .groupby(\"_email_lower\")\n    .apply(summarize_group)\n    .reset_index()\n    .rename(columns={\"_email_lower\": \"email_lower\"})\n    .sort_values([\"status\", \"rows_in_group\"], ascending=[True, False])\n)\n\n# 5) Focused views you’ll care about:\nconflicts = dup_summary[dup_summary[\"status\"] == \"conflict_birthdays\"]\nsame_bday = dup_summary[dup_summary[\"status\"] == \"same_birthday\"]\nno_bday   = dup_summary[dup_summary[\"status\"] == \"no_birthday_data\"]\n\nprint(f\"\\nGroups with conflicting birthdays: {len(conflicts)}\")\nprint(f\"Groups with the same (non-null) birthday: {len(same_bday)}\")\nprint(f\"Groups with no birthday data: {len(no_bday)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T06:24:31.570040Z","iopub.execute_input":"2025-08-14T06:24:31.570420Z","iopub.status.idle":"2025-08-14T06:24:31.731146Z","shell.execute_reply.started":"2025-08-14T06:24:31.570369Z","shell.execute_reply":"2025-08-14T06:24:31.727091Z"}},"outputs":[{"name":"stdout","text":"Total rows: 1515\nDuplicate emails (case-insensitive): 174\n\nGroups with conflicting birthdays: 35\nGroups with the same (non-null) birthday: 139\nGroups with no birthday data: 0\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3822156598.py:39: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(summarize_group)\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"EMAIL = \"E-mail\"\nFNAME = \"First Name\"\nLNAME = \"Last Name\"\nBDAY  = \"Birthday\"\nPHONE = \"Phone Number\"\n\ndef ascii_fold(s: str) -> str:\n    s = (\"\" if s is None else str(s)).strip().lower()\n    s = unicodedata.normalize(\"NFKD\", s)\n    s = \"\".join(c for c in s if not unicodedata.combining(c))  # strip diacritics (Michał -> michal)\n    s = re.sub(r\"[^a-z\\s'-]\", \" \", s)  # keep letters, space, ', -\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\ndef phone_digits(s: str) -> str:\n    return re.sub(r\"\\D+\", \"\", \"\" if s is None else str(s))\n\n# Work copy\nd = df_trim.copy()\n\n# Normalizations\nd[\"_email_lower\"]  = d[EMAIL].astype(str).str.strip().str.lower()\nd[\"_first_norm\"]   = d[FNAME].map(ascii_fold)\nd[\"_last_norm\"]    = d[LNAME].map(ascii_fold)\nd[\"_phone_norm\"]   = d[PHONE].map(phone_digits) if PHONE in d.columns else \"\"\nd[\"_dob\"]          = pd.to_datetime(d[BDAY], errors=\"coerce\").dt.date\n\n# Build a conservative person key:\n#   Prefer (last, first, dob); else (last, first, phone>=7d); else (last, first) and mark for review.\ndef make_person_key(row):\n    if pd.notna(row[\"_dob\"]):\n        return f\"{row['_last_norm']}|{row['_first_norm']}|{row['_dob']}\"\n    if row[\"_phone_norm\"] and len(row[\"_phone_norm\"]) >= 7:\n        return f\"{row['_last_norm']}|{row['_first_norm']}|{row['_phone_norm']}\"\n    return f\"{row['_last_norm']}|{row['_first_norm']}\"\n\nd[\"_person_key\"] = d.apply(make_person_key, axis=1)\nd[\"_has_strong_key\"] = d[\"_person_key\"].str.contains(r\"\\|\\d{4}-\\d{2}-\\d{2}$\") | d[\"_person_key\"].str.contains(r\"\\|\\d{7,}$\")\n\n# Consider only non-empty emails\nd = d[d[\"_email_lower\"].str.len() > 0]\n\n# Emails that appear >1 (shared)\nvc_email = d[\"_email_lower\"].value_counts()\nshared_email_keys = vc_email[vc_email > 1].index\n\nshared = d[d[\"_email_lower\"].isin(shared_email_keys)].copy()\n\n# Summary per email: how many distinct people under that email (by our person_key)\nemail_person_summary = (\n    shared.groupby(\"_email_lower\")\n    .agg(\n        rows_in_group = (EMAIL, \"size\"),\n        distinct_people = (\"_person_key\", \"nunique\"),\n        strong_keys = (\"_has_strong_key\", \"sum\")\n    )\n    .sort_values([\"distinct_people\", \"rows_in_group\"], ascending=[False, False])\n    .reset_index()\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T06:24:31.732411Z","iopub.execute_input":"2025-08-14T06:24:31.732723Z","iopub.status.idle":"2025-08-14T06:24:31.835132Z","shell.execute_reply.started":"2025-08-14T06:24:31.732699Z","shell.execute_reply":"2025-08-14T06:24:31.833129Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"EMAIL = \"E-mail\"\nFNAME = \"First Name\"\nLNAME = \"Last Name\"\nBDAY  = \"Birthday\"\nPHONE = \"Phone Number\"\nSAA   = \"Select an Address\"\nADR   = \"Adres korespondencyjny\"\n\ndef ascii_fold(s: str) -> str:\n    s = (\"\" if s is None else str(s)).strip().lower()\n    s = unicodedata.normalize(\"NFKD\", s)\n    s = \"\".join(c for c in s if not unicodedata.combining(c))  # strip diacritics\n    s = re.sub(r\"[^a-z\\s'-]\", \" \", s)  # keep letters, space, ', -\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\ndef phone_digits(s: str) -> str:\n    return re.sub(r\"\\D+\", \"\", \"\" if s is None else str(s))\n\ndef nonempty_count(row, cols):\n    cnt = 0\n    for c in cols:\n        v = row.get(c, None)\n        if pd.notna(v) and str(v).strip() != \"\":\n            cnt += 1\n    return cnt\n\ndef choose_best(grp, value_cols):\n    \"\"\"Pick the 'golden' row: most non-empty fields; then first-in-order tie-breaker.\"\"\"\n    g = grp.copy()\n    g[\"_comp\"] = g.apply(lambda r: nonempty_count(r, value_cols), axis=1)\n    g = g.sort_values(by=[\"_comp\"], ascending=[False], kind=\"stable\")\n    return g.iloc[0]\n\n# Work copy + normalizations\nd = df_trim.copy()\nd[\"_email_lower\"] = d[EMAIL].astype(str).str.strip().str.lower()\nd[\"_first_norm\"]  = d[FNAME].map(ascii_fold)\nd[\"_last_norm\"]   = d[LNAME].map(ascii_fold)\nd[\"_dob\"]         = pd.to_datetime(d[BDAY], errors=\"coerce\").dt.date\nd[\"_phone_norm\"]  = d[PHONE].map(phone_digits) if PHONE in d.columns else \"\"\n\n# --- Step 1: within (email, name), drop older birthdays; keep newest DOB only ---\nvalue_cols = [FNAME, LNAME, BDAY, EMAIL, PHONE, SAA, ADR]\ngroups_en = d.groupby([\"_email_lower\",\"_last_norm\",\"_first_norm\"], dropna=False)\n\nrows_after_dob = []\nremoved_older_dobs = 0\n\nfor key, grp in groups_en:\n    # newest DOB within this email+name group\n    newest = grp[\"_dob\"].dropna().max() if grp[\"_dob\"].notna().any() else pd.NaT\n\n    if pd.isna(newest):\n        # no DOBs in this group -> keep one best row; duplicates will be handled by person_key next\n        rows_after_dob.append(choose_best(grp, value_cols))\n    else:\n        # keep only rows that match the newest DOB\n        keep = grp[grp[\"_dob\"] == newest]\n        removed_older_dobs += (len(grp) - len(keep))\n        # if multiple with same newest DOB, pick the best one here\n        rows_after_dob.append(choose_best(keep, value_cols))\n\nd1 = pd.DataFrame(rows_after_dob).reset_index(drop=True)\n\n# --- Step 2: one row per person_key globally (not just within an email) ---\ndef make_person_key(row):\n    # Prefer DOB when present; else phone>=7 digits; else name only\n    if pd.notna(row[\"_dob\"]):\n        return f\"{row['_last_norm']}|{row['_first_norm']}|{row['_dob']}\"\n    if row[\"_phone_norm\"] and len(row[\"_phone_norm\"]) >= 7:\n        return f\"{row['_last_norm']}|{row['_first_norm']}|{row['_phone_norm']}\"\n    return f\"{row['_last_norm']}|{row['_first_norm']}\"\n\nd1[\"_person_key\"] = d1.apply(make_person_key, axis=1)\n\nrows_final = []\nfor pk, grp in d1.groupby(\"_person_key\", dropna=False):\n    rows_final.append(choose_best(grp, value_cols))\n\nmembers_clean = pd.DataFrame(rows_final).reset_index(drop=True)\n\n# --- Summaries ---\nprint(\"Original rows:\", len(df_trim))\nprint(\"After dropping older DOBs within same email+name:\", len(d1), f\"(removed {removed_older_dobs})\")\nprint(\"Final distinct persons (one row/person):\", len(members_clean))\nprint(\"Emails that remain shared (multiple persons per email):\",\n      int(members_clean[\"_email_lower\"].value_counts().gt(1).sum()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T06:24:31.836691Z","iopub.execute_input":"2025-08-14T06:24:31.837064Z","iopub.status.idle":"2025-08-14T06:24:35.967794Z","shell.execute_reply.started":"2025-08-14T06:24:31.837034Z","shell.execute_reply":"2025-08-14T06:24:35.966757Z"}},"outputs":[{"name":"stdout","text":"Original rows: 1515\nAfter dropping older DOBs within same email+name: 1302 (removed 30)\nFinal distinct persons (one row/person): 1276\nEmails that remain shared (multiple persons per email): 15\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"SAA = \"Select an Address\"\nADR = \"Adres korespondencyjny\"\nEXPORT_PATH = \"/kaggle/working/source_of_truth.csv\"\n\n# 1) choose the dataset to export\nbase = members_clean if 'members_clean' in globals() else df_trim.copy()\nout = base.copy()\n\n# 2) drop the ADR column\nif ADR in out.columns:\n    out.drop(columns=[ADR], inplace=True)\n\n# 3) drop helper/engineered columns (anything starting with \"_\")\nhelper_cols = [c for c in out.columns if c.startswith(\"_\")]\nout.drop(columns=helper_cols, inplace=True, errors=\"ignore\")\n\n# 4) put key columns up front; keep any remaining columns afterward\nfront = [\"First Name\", \"Last Name\", \"Birthday\", \"E-mail\", \"Phone Number\", SAA]\nfront_existing = [c for c in front if c in out.columns]\nout = out[front_existing + [c for c in out.columns if c not in front_existing]]\n\n# 5) write the CSV\nout.to_csv(EXPORT_PATH, index=False, encoding=\"utf-8-sig\")\n\nprint(f\"Wrote: {EXPORT_PATH}\")\nprint(f\"Rows: {len(out)} | Columns: {len(out.columns)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T06:24:35.970289Z","iopub.execute_input":"2025-08-14T06:24:35.971458Z","iopub.status.idle":"2025-08-14T06:24:36.002844Z","shell.execute_reply.started":"2025-08-14T06:24:35.971372Z","shell.execute_reply":"2025-08-14T06:24:36.001339Z"}},"outputs":[{"name":"stdout","text":"Wrote: /kaggle/working/source_of_truth.csv\nRows: 1276 | Columns: 6\n","output_type":"stream"}],"execution_count":72}]}